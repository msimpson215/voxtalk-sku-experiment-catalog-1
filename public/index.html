<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>VoxTalk (Demo)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="app">
    <h1 class="demo-label">Demo</h1>
    <h2 class="talk-label">Talk to VoxTalkâ„¢</h2>

    <button id="pttBtn" aria-pressed="false">
      <span class="pulse-container">
        <span class="ring"></span>
        <span class="ring"></span>
        <span class="ring"></span>
        <span class="ring"></span>
        <span class="ring"></span>
      </span>
    </button>
    <div class="hint">Click to Talk</div>

    <div id="answer"></div>

    <audio id="remote" autoplay playsinline></audio>

    <p class="powered">Powered by VoxTalkâ„¢</p>
  </div>

  <script>
    const pttBtn   = document.getElementById('pttBtn');
    const answerEl = document.getElementById('answer');
    const rtAudio  = document.getElementById('remote');

    function appendLine(role, text) {
      if (!text.trim()) return;
      const div = document.createElement('div');
      div.className = 'line';
      div.innerHTML = `<span class="${role}">${role==='me'?'You:':'AI:'}</span>
                       <span class="text">${text}</span>`;
      answerEl.appendChild(div);
      answerEl.scrollTop = answerEl.scrollHeight;
    }

    function setSpeaking(on) {
      if (on) {
        pttBtn.classList.add('speaking');
      } else {
        pttBtn.classList.remove('speaking');
      }
    }

    async function initRealtime() {
      try {
        const s = await fetch("/session", { method:"POST" });
        const sessionData = await s.json();

        if (!s.ok) {
          appendLine("ai", `Session error: ${sessionData?.error || "unknown error"}`);
          return;
        }

        const { client_secret, model, voice } = sessionData;
        if (!client_secret) {
          appendLine("ai", "No client_secret returned from server.");
          return;
        }

        const pc = new RTCPeerConnection();

        // ðŸŽ¯ More sensitive analyser + longer cutoff delay
        pc.ontrack = (ev)=> {
          const remoteStream = ev.streams[0];
          rtAudio.srcObject = remoteStream;

          const audioCtx = new (window.AudioContext || window.webkitAudioContext)();
          const source = audioCtx.createMediaStreamSource(remoteStream);
          const analyser = audioCtx.createAnalyser();
          analyser.fftSize = 512;
          const dataArray = new Float32Array(analyser.fftSize);
          source.connect(analyser);

          let aiSpeaking = false;
          let lastAbove = 0;

          function monitor() {
            analyser.getFloatTimeDomainData(dataArray);
            let sum = 0;
            for (let i=0;i<dataArray.length;i++) sum += dataArray[i]*dataArray[i];
            const rms = Math.sqrt(sum / dataArray.length);
            const now = performance.now();

            if (rms > 0.01) { // lowered threshold for easier trigger
              lastAbove = now;
              if (!aiSpeaking) {
                aiSpeaking = true;
                setSpeaking(true);
              }
            } else if (aiSpeaking && now - lastAbove > 500) { // longer cutoff
              aiSpeaking = false;
              setSpeaking(false);
            }
            requestAnimationFrame(monitor);
          }
          monitor();
        };

        pc.ondatachannel = (ev) => {
          const ch = ev.channel;
          ch.onmessage = handleEventMessage;
        };

        const dc = pc.createDataChannel("events");
        dc.onmessage = handleEventMessage;

        function handleEventMessage(e){
          try {
            const evt = JSON.parse(e.data);
            if (evt.type === "response.message.delta") {
              const chunk = evt.delta.map(d=>d.content?.[0]?.text||"").join("");
              if (chunk) appendLine("ai", chunk);
            }
            if (evt.type === "response.audio_transcript.delta") {
              const transcript = evt.delta.map(d => d.text || "").join("");
              if (transcript) appendLine("me", transcript);
              // halos handled by analyser, not here
            }
          } catch(err) { console.error("Parse error:", e.data, err); }
        }

        const mic = await navigator.mediaDevices.getUserMedia({ audio:true });
        const micTrack = mic.getTracks()[0];
        micTrack.enabled = false;
        pc.addTrack(micTrack, mic);

        const offer = await pc.createOffer({ offerToReceiveAudio:true });
        await pc.setLocalDescription(offer);

        const r = await fetch(
          `https://api.openai.com/v1/realtime?model=${encodeURIComponent(model)}&voice=${voice}`,
          {
            method:"POST",
            headers: {
              "Authorization":`Bearer ${client_secret.value || client_secret}`,
              "Content-Type":"application/sdp",
              "OpenAI-Beta":"realtime=v1"
            },
            body: offer.sdp
          }
        );

        if (!r.ok) {
          appendLine("ai", `Realtime error ${r.status}: ${await r.text()}`);
          return;
        }

        const answer = { type:"answer", sdp: await r.text() };
        await pc.setRemoteDescription(answer);

        let talking = false;
        pttBtn.onclick = () => {
          talking = !talking;
          micTrack.enabled = talking;
          pttBtn.setAttribute("aria-pressed", talking ? "true" : "false");
          appendLine("me", talking ? "(Listening...)" : "(Stopped)");
        };

      } catch(err) {
        appendLine("ai", "Init failed: " + (err?.message || err));
      }
    }
    initRealtime();
  </script>
</body>
</html>
