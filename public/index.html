<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>VoxTalk</title>
  <link rel="stylesheet" href="style.css" />
</head>
<body>
  <div class="container">
    <!-- Talk button -->
    <button id="talk-btn" class="talk-btn">
      üéôÔ∏è Talk / Stop
    </button>

    <!-- Transcript box -->
    <div id="transcript" class="transcript">
      Your conversation will appear here
    </div>
  </div>

  <script type="module">
    let ws;
    let mediaRecorder;
    let audioChunks = [];
    let isRecording = false;
    let audioCtx;
    let responseTimer;

    const talkBtn = document.getElementById("talk-btn");
    const transcriptEl = document.getElementById("transcript");

    async function toggleRecording() {
      if (!isRecording) await startRecording();
      else stopRecording();
    }

    async function startRecording() {
      try {
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        mediaRecorder = new MediaRecorder(stream);
        audioChunks = [];

        ws = await connectWS();
        if (!ws) console.warn("‚ö†Ô∏è No session token ‚Äî fallback only.");

        if (ws) {
          ws.onopen = () => console.log("‚úÖ WS connected");
          ws.onmessage = (ev) => {
            try {
              const msg = JSON.parse(ev.data);

              // ‚úÖ NEW: print VoxTalk text when received
              if (msg.type === "output_text.delta") {
                transcriptEl.innerHTML += `<div><b>VoxTalk:</b> ${msg.delta}</div>`;
                transcriptEl.scrollTop = transcriptEl.scrollHeight;
              }

              if (msg.type === "output_audio.delta") {
                playAudioChunk(msg.delta);
              }

              if (msg.type === "response.error") {
                console.error("Realtime error:", msg.error?.message);
              }
            } catch {}
          };
          ws.onerror = (e) => console.error("WS error:", e);
          ws.onclose = () => console.log("üîå WS closed ‚Äî may fallback if no deltas.");
        }

        mediaRecorder.ondataavailable = (e) => {
          if (e.data.size > 0) audioChunks.push(e.data);
        };

        mediaRecorder.onstop = () => {
          const blob = new Blob(audioChunks, { type: "audio/webm" });
          blob.arrayBuffer().then((buf) => {
            if (ws && ws.readyState === WebSocket.OPEN) {
              ws.send(JSON.stringify({ type: "input_audio_buffer.append", audio: arrayBufferToBase64(buf) }));
              ws.send(JSON.stringify({ type: "input_audio_buffer.commit" }));
              ws.send(JSON.stringify({
                type: "response.create",
                response: {
                  modalities: ["audio", "text"],
                  instructions: "Respond naturally and include text transcript.",
                  audio: { voice: "alloy" }
                }
              }));

              clearTimeout(responseTimer);
              responseTimer = setTimeout(() => {
                console.warn("Realtime silent ‚Äî using fallback.");
                fallbackSpeak([{ role: "user", content: "(user spoke)" }]);
              }, 3000);
            } else {
              console.warn("Realtime not available ‚Äî fallback now.");
              fallbackSpeak([{ role: "user", content: "(user spoke)" }]);
            }
          });
        };

        mediaRecorder.start();
        transcriptEl.innerHTML += `<div><b>You:</b> <i>Listening‚Ä¶</i></div>`;
        isRecording = true;
        talkBtn.textContent = "üõë Stop";
      } catch (err) {
        console.error("Microphone error:", err.message);
      }
    }

    function stopRecording() {
      if (mediaRecorder && mediaRecorder.state !== "inactive") mediaRecorder.stop();
      isRecording = false;
      talkBtn.textContent = "üéôÔ∏è Talk / Stop";
    }

    function arrayBufferToBase64(buffer) {
      let binary = "";
      const bytes = new Uint8Array(buffer);
      for (let i = 0; i < bytes.byteLength; i++) binary += String.fromCharCode(bytes[i]);
      return btoa(binary);
    }

    function playAudioChunk(base64Data) {
      if (!audioCtx) audioCtx = new AudioContext();
      const audioData = Uint8Array.from(atob(base64Data), (c) => c.charCodeAt(0)).buffer;
      audioCtx.decodeAudioData(audioData).then((decoded) => {
        const source = audioCtx.createBufferSource();
        source.buffer = decoded;
        source.connect(audioCtx.destination);
        source.start();
      });
    }

    talkBtn.addEventListener("click", toggleRecording);

    async function connectWS() {
      try {
        const r = await fetch("/session", { method: "POST" });
        const data = await r.json();
        if (data.error) {
          console.error(`API Error: ${data.message}`);
          return null;
        }
        const wsUrl = `wss://api.openai.com/v1/realtime?session=${
          data.token.split("session=")[1] || data.token
        }`;
        const sock = new WebSocket(wsUrl, ["realtime"]);
        sock.binaryType = "arraybuffer";
        return sock;
      } catch (err) {
        console.error(`Session fetch error: ${err.message}`);
        return null;
      }
    }

    async function fallbackSpeak(messages) {
      try {
        const r = await fetch("/chat-tts", {
          method: "POST",
          headers: { "Content-Type": "application/json" },
          body: JSON.stringify({ messages }),
        });
        const textHeader = decodeURIComponent(r.headers.get("x-text") || "");
        transcriptEl.innerHTML += `<div><b>VoxTalk (fallback):</b> ${textHeader}</div>`;
        const blob = await r.blob();
        const url = URL.createObjectURL(blob);
        const audio = new Audio(url);
        await audio.play();
      } catch (err) {
        console.error("Fallback failed ‚Äî check server logs.", err.message);
      }
    }
  </script>
</body>
</html>
