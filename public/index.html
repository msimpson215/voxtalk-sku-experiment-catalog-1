<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>VoxTalk (Demo)</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="style.css">
</head>
<body>
  <div class="app">
    <h1 class="demo-label">Demo</h1>
    <h2 class="talk-label">Talk to VoxTalk™</h2>

    <!-- Main Talk Button with ring container -->
    <button id="pttBtn" aria-pressed="false">
      <span class="pulse-container">
        <span class="ring"></span>
        <span class="ring"></span>
        <span class="ring"></span>
        <span class="ring"></span>
        <span class="ring"></span>
      </span>
    </button>
    <div class="hint">Click to Talk</div>

    <!-- Transcript Box -->
    <div id="answer"></div>

    <!-- Remote Audio -->
    <audio id="remote" autoplay playsinline></audio>

    <p class="powered">Powered by VoxTalk™</p>
  </div>

  <script>
    const pttBtn   = document.getElementById('pttBtn');
    const answerEl = document.getElementById('answer');
    const rtAudio  = document.getElementById('remote');

    // --- transcript helpers (unchanged from your working flow) ---
    function appendLine(role, text) {
      if (!text || !text.trim()) return;
      const div = document.createElement('div');
      div.className = 'line';
      div.innerHTML = `<span class="${role}">${role==='me'?'You:':'AI:'}</span>
                       <span class="text">${text}</span>`;
      answerEl.appendChild(div);
      answerEl.scrollTop = answerEl.scrollHeight;
    }

    // --- HALOS: driven by AI audio level (not deltas, not button) ---
    let audioCtx, analyser, dataArray, meterRAF, meterReady = false;
    let aiSpeaking = false;
    let lastVoiceTs = 0;
    let remoteStream = null;

    function halosOn() {
      if (!aiSpeaking) {
        aiSpeaking = true;
        pttBtn.classList.add('speaking');
      }
    }
    function halosOff() {
      if (aiSpeaking) {
        aiSpeaking = false;
        pttBtn.classList.remove('speaking');
      }
    }

    function startMeterIfReady() {
      if (meterReady || !remoteStream) return;
      // must be called after a user gesture (the button click covers this)
      audioCtx = new (window.AudioContext || window.webkitAudioContext)();
      const src = audioCtx.createMediaStreamSource(remoteStream);
      analyser = audioCtx.createAnalyser();
      analyser.fftSize = 1024;
      src.connect(analyser);
      dataArray = new Float32Array(analyser.fftSize);
      meterReady = true;
      meterLoop();
    }

    function meterLoop() {
      analyser.getFloatTimeDomainData(dataArray);
      let sum = 0;
      for (let i = 0; i < dataArray.length; i++) {
        const x = dataArray[i];
        sum += x * x;
      }
      const rms = Math.sqrt(sum / dataArray.length);

      // Hysteresis thresholds + short release so halos don’t blink
      const THRESH_ON  = 0.02;   // turn on above this
      const THRESH_OFF = 0.015;  // turn off below this (with hold)
      const now = performance.now();

      if (rms > THRESH_ON) {
        lastVoiceTs = now;
        halosOn();
      } else if (aiSpeaking && (now - lastVoiceTs) > 250) {
        halosOff();
      }

      meterRAF = requestAnimationFrame(meterLoop);
    }

    async function initRealtime() {
      try {
        const s = await fetch("/session", { method:"POST" });
        const sessionData = await s.json();

        if (!s.ok) {
          appendLine("ai", `Session error: ${sessionData?.error || "unknown error"}`);
          return;
        }

        const { client_secret, model, voice } = sessionData;
        if (!client_secret) {
          appendLine("ai", "No client_secret returned from server.");
          return;
        }

        const pc = n
